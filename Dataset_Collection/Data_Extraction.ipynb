{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j7Y9SOEoOI8R"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_page(url):\n",
        "    # Send a request to the URL\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    questions = soup.find_all('p', class_='question')\n",
        "    data = []\n",
        "    for question in questions:\n",
        "        answer = question.find_next_sibling('p', class_='answer')\n",
        "        if answer:\n",
        "            data.append({\n",
        "                'question': question.text.strip(),\n",
        "                'answer': answer.text.strip()\n",
        "            })\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "dERIxpisO8F1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(urls):\n",
        "    all_data = []\n",
        "    for url in urls:\n",
        "        all_data.extend(scrape_page(url))\n",
        "        if len(all_data) >= 100:  # Stop once we collect at least 100 Q&A pairs\n",
        "            break\n",
        "    return all_data[:100]  # Return only the first 100 entries\n"
      ],
      "metadata": {
        "id": "Bj0tg6gmPBXl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_json(data, filename='data.json'):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n"
      ],
      "metadata": {
        "id": "ZN_G1cs0PCRL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    'http://example.com/faq1',\n",
        "    'http://example.com/faq2',\n",
        "]\n",
        "\n",
        "# Running the main function\n",
        "extracted_data = main(urls)\n",
        "\n",
        "# Saving the data to a JSON file\n",
        "save_to_json(extracted_data)\n"
      ],
      "metadata": {
        "id": "2BXvzxIPPGGH"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}